"""
å¯¹è¯å¼å›¾ç‰‡æœç´¢è„šæœ¬
ç”¨æˆ·å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ŒAIåŠ©æ‰‹å¸®åŠ©æ‰¾åˆ°ç›¸å…³å›¾ç‰‡
"""
import os
import glob
import json
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI

# è½½å…¥ç¯å¢ƒå˜é‡
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(api_key=openai_api_key)

def filename_to_url(filename):
    """å°†æ–‡ä»¶åè½¬æ¢å›åŸå§‹URL"""
    name_without_ext = filename.replace('.html', '')
    url_path = name_without_ext.replace('_', '/')
    
    if url_path.startswith('www.'):
        return f"https://{url_path}"
    else:
        return f"https://{url_path}"

def get_image_format(url):
    """è·å–å›¾ç‰‡æ ¼å¼"""
    url_lower = url.lower()
    if any(ext in url_lower for ext in ['.jpg', '.jpeg']):
        return 'jpg'
    elif '.png' in url_lower:
        return 'png'
    elif '.svg' in url_lower:
        return 'svg'
    elif '.webp' in url_lower:
        return 'webp'
    elif '.gif' in url_lower:
        return 'gif'
    else:
        return 'unknown'

def extract_context_from_source(source_tag):
    """ä»sourceæ ‡ç­¾æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    context_parts = []
    
    media_attr = source_tag.get('media', '')
    if media_attr:
        context_parts.append(f"Media: {media_attr}")
    
    picture = source_tag.find_parent('picture')
    if picture:
        img_in_picture = picture.find('img')
        if img_in_picture:
            alt_text = img_in_picture.get('alt', '')
            title_text = img_in_picture.get('title', '')
            class_attr = ' '.join(img_in_picture.get('class', []))
            
            if alt_text:
                context_parts.append(f"Alt: {alt_text}")
            if title_text:
                context_parts.append(f"Title: {title_text}")
            if class_attr:
                context_parts.append(f"Class: {class_attr}")
    
    parent = source_tag.parent
    if parent:
        parent_text = parent.get_text(strip=True)
        if parent_text and len(parent_text) < 300:
            context_parts.append(f"Parent text: {parent_text}")
    
    return " | ".join(context_parts) if context_parts else str(source_tag)

def extract_context(img_tag):
    """æå–å›¾ç‰‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    context_parts = []
    
    alt_text = img_tag.get('alt', '')
    title_text = img_tag.get('title', '')
    class_attr = ' '.join(img_tag.get('class', []))
    
    if alt_text:
        context_parts.append(f"Alt: {alt_text}")
    if title_text:
        context_parts.append(f"Title: {title_text}")
    if class_attr:
        context_parts.append(f"Class: {class_attr}")
    
    parent = img_tag.parent
    if parent:
        parent_text = parent.get_text(strip=True)
        if parent_text and len(parent_text) < 200:
            context_parts.append(f"Parent text: {parent_text}")
    
    return " | ".join(context_parts) if context_parts else str(img_tag)

def process_html_file(html_file_path, source_url):
    """å¤„ç†å•ä¸ªHTMLæ–‡ä»¶ï¼Œè¿”å›æ–‡æ¡£åˆ—è¡¨"""
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html = f.read()
    except UnicodeDecodeError:
        try:
            with open(html_file_path, 'r', encoding='latin-1') as f:
                html = f.read()
        except:
            return []
    
    soup = BeautifulSoup(html, 'html.parser')
    parsed_url = urlparse(source_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    docs = []
    all_imgs = soup.find_all('img')
    all_sources = soup.find_all('source')
    
    # å¤„ç†imgæ ‡ç­¾
    for img in all_imgs:
        raw = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-srcset')
        if not raw:
            continue
        
        for part in raw.split(','):
            u = part.strip().split(' ')[0]
            if u.startswith('//'):
                u = 'https:' + u
            elif u.startswith('/'):
                u = urljoin(base_url, u)
            elif not u.startswith('http'):
                u = urljoin(source_url, u)
            
            img_format = get_image_format(u)
            context = extract_context(img)
            
            alt_text = img.get('alt', '')
            title_text = img.get('title', '')
            class_attr = ' '.join(img.get('class', []))
            
            doc = Document(
                page_content=f"Alt: {alt_text} | Title: {title_text} | Class: {class_attr} | Context: {context}",
                metadata={
                    'img_url': u,
                    'img_format': img_format,
                    'alt_text': alt_text,
                    'title': title_text,
                    'class': class_attr,
                    'source_type': 'img',
                    'source_url': source_url,
                    'source_file': os.path.basename(html_file_path)
                }
            )
            docs.append(doc)
    
    # å¤„ç†sourceæ ‡ç­¾
    for source in all_sources:
        srcset = source.get('srcset', '')
        if not srcset:
            continue
        
        for part in srcset.split(','):
            url_part = part.strip().split(' ')[0]
            if url_part.startswith('/'):
                url_part = urljoin(base_url, url_part)
            elif not url_part.startswith('http'):
                url_part = urljoin(source_url, url_part)
            
            img_format = get_image_format(url_part)
            context = extract_context_from_source(source)
            
            picture = source.find_parent('picture')
            alt_text = ''
            title_text = ''
            class_attr = ''
            
            if picture:
                img_in_picture = picture.find('img')
                if img_in_picture:
                    alt_text = img_in_picture.get('alt', '')
                    title_text = img_in_picture.get('title', '')
                    class_attr = ' '.join(img_in_picture.get('class', []))
            
            doc = Document(
                page_content=f"Alt: {alt_text} | Title: {title_text} | Class: {class_attr} | Context: {context}",
                metadata={
                    'img_url': url_part,
                    'img_format': img_format,
                    'alt_text': alt_text,
                    'title': title_text,
                    'class': class_attr,
                    'source_type': 'source',
                    'media': source.get('media', ''),
                    'source_url': source_url,
                    'source_file': os.path.basename(html_file_path)
                }
            )
            docs.append(doc)
    
    return docs

def load_html_folder(folder_path):
    """åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰HTMLæ–‡ä»¶"""
    print(f"åŠ è½½HTMLæ–‡ä»¶å¤¹: {folder_path}")
    
    if not os.path.exists(folder_path):
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    html_pattern = os.path.join(folder_path, "*.html")
    html_files = glob.glob(html_pattern)
    
    if not html_files:
        raise ValueError(f"åœ¨æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶: {folder_path}")
    
    print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    all_docs = []
    
    for html_file in html_files:
        filename = os.path.basename(html_file)
        source_url = filename_to_url(filename)
        docs = process_html_file(html_file, source_url)
        all_docs.extend(docs)
    
    print(f"æ€»å…±å¤„ç†äº† {len(all_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    return all_docs

def search_images_with_dedup(chroma_db, query, format_filter=None, max_results=5):
    """æœç´¢å›¾ç‰‡å¹¶å»é‡"""
    # æ‰§è¡Œæœç´¢
    results = chroma_db.similarity_search_with_score(query, k=50)
    
    processed_results = []
    
    for doc, score in results:
        img_format = doc.metadata['img_format']
        
        # åº”ç”¨æ ¼å¼è¿‡æ»¤
        if format_filter and img_format not in format_filter:
            continue
        
        # è®¡ç®—Altæ–‡æœ¬åŒ¹é…åº¦
        alt_text = doc.metadata.get('alt_text', '').lower()
        title_text = doc.metadata.get('title', '').lower()
        query_lower = query.lower()
        
        alt_match_score = 0
        if alt_text and query_lower in alt_text:
            alt_match_score += 2.0
        if title_text and query_lower in title_text:
            alt_match_score += 1.0
        
        query_words = query_lower.split()
        for word in query_words:
            if len(word) > 2:
                if word in alt_text:
                    alt_match_score += 0.5
                if word in title_text:
                    alt_match_score += 0.3
        
        img_info = {
            'url': doc.metadata['img_url'],
            'format': img_format,
            'alt_text': doc.metadata.get('alt_text', ''),
            'title': doc.metadata.get('title', ''),
            'source_type': doc.metadata['source_type'],
            'media': doc.metadata.get('media', ''),
            'score': score,
            'alt_match_score': alt_match_score,
            'source_url': doc.metadata['source_url'],
            'source_file': doc.metadata['source_file'],
            'context': doc.page_content
        }
        processed_results.append(img_info)
    
    # å»é‡é€»è¾‘
    def normalize_alt_text(alt_text):
        if not alt_text:
            return ""
        import re
        normalized = alt_text.lower().strip()
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized
    
    def should_prefer_by_alt(img1, img2):
        # JPG > PNG > å…¶ä»–
        if img1['format'] != img2['format']:
            format_priority = {'jpg': 3, 'png': 2, 'webp': 1, 'svg': 0}
            priority1 = format_priority.get(img1['format'], 0)
            priority2 = format_priority.get(img2['format'], 0)
            if priority1 != priority2:
                return priority1 > priority2
        
        # AltåŒ¹é…åº¦ä¼˜å…ˆ
        if img1['alt_match_score'] != img2['alt_match_score']:
            return img1['alt_match_score'] > img2['alt_match_score']
        
        # ç›¸ä¼¼åº¦åˆ†æ•°
        return img1['score'] < img2['score']
    
    # Altæ–‡æœ¬å»é‡
    seen_alt_texts = {}
    final_results = []
    
    for img in processed_results:
        alt_text = normalize_alt_text(img['alt_text'])
        
        if not alt_text:
            final_results.append(img)
            continue
        
        if alt_text in seen_alt_texts:
            existing_img = seen_alt_texts[alt_text]
            if should_prefer_by_alt(img, existing_img):
                seen_alt_texts[alt_text] = img
                final_results = [r for r in final_results if normalize_alt_text(r['alt_text']) != alt_text]
                final_results.append(img)
        else:
            seen_alt_texts[alt_text] = img
            final_results.append(img)
    
    # æ’åº
    if not format_filter:
        final_results.sort(key=lambda x: (
            -x['alt_match_score'],
            x['format'] not in ['jpg', 'png'],
            x['format'] != 'jpg',
            x['score']
        ))
    else:
        final_results.sort(key=lambda x: (-x['alt_match_score'], x['score']))
    
    return final_results[:max_results]

def parse_user_query_with_ai(user_message):
    """ä½¿ç”¨AIè§£æç”¨æˆ·æŸ¥è¯¢ï¼Œæå–æœç´¢è¯å’Œæ ¼å¼è¦æ±‚"""
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªå›¾ç‰‡æœç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šç”¨è‡ªç„¶è¯­è¨€æè¿°ä»–ä»¬æƒ³è¦çš„å›¾ç‰‡ï¼Œä½ éœ€è¦æå–å…³é”®çš„æœç´¢ä¿¡æ¯ã€‚

è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œè¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«ï¼š
1. search_query: ç”¨äºæœç´¢çš„å…³é”®è¯ï¼ˆè‹±æ–‡ï¼Œé€‚åˆå›¾ç‰‡Altæ–‡æœ¬æœç´¢ï¼‰
2. format_filter: å›¾ç‰‡æ ¼å¼è¦æ±‚ï¼ˆå¦‚æœç”¨æˆ·æŒ‡å®šäº†JPGã€PNGç­‰æ ¼å¼ï¼Œå¦åˆ™ä¸ºnullï¼‰
3. response_message: ç»™ç”¨æˆ·çš„å‹å¥½å›å¤ï¼Œè¯´æ˜ä½ ç†è§£äº†ä»€ä¹ˆ

ç¤ºä¾‹ï¼š
ç”¨æˆ·ï¼š"æˆ‘æƒ³è¦iPadç›¸å…³çš„JPGå›¾ç‰‡"
è¿”å›ï¼š{"search_query": "iPad", "format_filter": ["jpg"], "response_message": "æˆ‘æ¥å¸®ä½ æ‰¾iPadç›¸å…³çš„JPGæ ¼å¼å›¾ç‰‡"}

ç”¨æˆ·ï¼š"ç»™æˆ‘çœ‹çœ‹è‹¹æœé“…ç¬”çš„ç…§ç‰‡"
è¿”å›ï¼š{"search_query": "Apple Pencil", "format_filter": null, "response_message": "æˆ‘æ¥ä¸ºä½ æœç´¢Apple Pencilçš„å›¾ç‰‡"}

ç”¨æˆ·ï¼š"æœ‰æ²¡æœ‰iPhoneæ‘„åƒå¤´çš„PNGå›¾ç‰‡ï¼Ÿ"
è¿”å›ï¼š{"search_query": "iPhone camera", "format_filter": ["png"], "response_message": "æˆ‘æ¥æŸ¥æ‰¾iPhoneæ‘„åƒå¤´çš„PNGæ ¼å¼å›¾ç‰‡"}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
    except Exception as e:
        print(f"AIè§£æé”™è¯¯: {e}")
        # é»˜è®¤å¤„ç†
        return {
            "search_query": user_message,
            "format_filter": None,
            "response_message": f"æˆ‘æ¥ä¸ºä½ æœç´¢å…³äº '{user_message}' çš„å›¾ç‰‡"
        }

def format_search_results_with_ai(search_results, user_query):
    """ä½¿ç”¨AIæ ¼å¼åŒ–æœç´¢ç»“æœ"""
    
    if not search_results:
        return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å›¾ç‰‡ã€‚"
    
    # å‡†å¤‡æœç´¢ç»“æœæ•°æ®
    results_data = []
    for i, img in enumerate(search_results, 1):
        results_data.append({
            "index": i,
            "alt_text": img['alt_text'],
            "format": img['format'].upper(),
            "url": img['url'],
            "source_url": img['source_url']
        })
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªå›¾ç‰‡æœç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·æœç´¢äº†å›¾ç‰‡ï¼Œä½ éœ€è¦ç”¨å‹å¥½çš„è¯­è¨€ä»‹ç»æœç´¢ç»“æœã€‚

è¦æ±‚ï¼š
1. ç”¨ç®€æ´å‹å¥½çš„è¯­è¨€ä»‹ç»æ‰¾åˆ°äº†ä»€ä¹ˆ
2. å¯¹æ¯å¼ å›¾ç‰‡ï¼Œåªæ˜¾ç¤ºï¼šå›¾ç‰‡URL å’Œ æ¥æºé¡µé¢
3. æ ¼å¼è¦æ¸…æ™°æ˜“è¯»
4. ä¸è¦æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚å¦‚alt_textç­‰

ç¤ºä¾‹æ ¼å¼ï¼š
æˆ‘ä¸ºä½ æ‰¾åˆ°äº†3å¼ ç›¸å…³å›¾ç‰‡ï¼š

ğŸ–¼ï¸ å›¾ç‰‡1ï¼š
å›¾ç‰‡é“¾æ¥ï¼šhttps://...
æ¥æºé¡µé¢ï¼šhttps://...

ğŸ–¼ï¸ å›¾ç‰‡2ï¼š
å›¾ç‰‡é“¾æ¥ï¼šhttps://...
æ¥æºé¡µé¢ï¼šhttps://...
"""

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ç”¨æˆ·æœç´¢ï¼š{user_query}\n\næœç´¢ç»“æœï¼š{json.dumps(results_data, ensure_ascii=False, indent=2)}"}
            ],
            temperature=0.3
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"AIæ ¼å¼åŒ–é”™è¯¯: {e}")
        # é»˜è®¤æ ¼å¼åŒ–
        result = f"æ‰¾åˆ°äº†{len(search_results)}å¼ ç›¸å…³å›¾ç‰‡ï¼š\n\n"
        for i, img in enumerate(search_results, 1):
            result += f"ğŸ–¼ï¸ å›¾ç‰‡{i}ï¼š\n"
            result += f"å›¾ç‰‡é“¾æ¥ï¼š{img['url']}\n"
            result += f"æ¥æºé¡µé¢ï¼š{img['source_url']}\n\n"
        return result

def main():
    # é…ç½®æ–‡ä»¶å¤¹è·¯å¾„
    crawled_folder = "crawled_pages_apple"
    
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¯¹è¯å¼å›¾ç‰‡æœç´¢ç³»ç»Ÿ...")
    
    # åŠ è½½HTMLæ–‡ä»¶
    try:
        all_docs = load_html_folder(crawled_folder)
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return
    
    if not all_docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡æ¡£ï¼")
        return
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    print("ğŸ“Š æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    chroma_all = Chroma.from_documents(
        all_docs,
        embedding=embeddings,
        collection_name='conversational_image_search'
    )
    
    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print("\nğŸ¤– AIå›¾ç‰‡æœç´¢åŠ©æ‰‹å·²å‡†å¤‡å°±ç»ªï¼")
    print("ğŸ’¬ ä½ å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ æƒ³è¦çš„å›¾ç‰‡ï¼Œä¾‹å¦‚ï¼š")
    print("   - 'æˆ‘æƒ³çœ‹iPadçš„å›¾ç‰‡'")
    print("   - 'æœ‰æ²¡æœ‰Apple Pencilçš„JPGç…§ç‰‡ï¼Ÿ'")
    print("   - 'ç»™æˆ‘æ‰¾ä¸€äº›iPhoneæ‘„åƒå¤´çš„å›¾ç‰‡'")
    print("   - 'è‹¹æœæ‰‹è¡¨çš„PNGå›¾ç‰‡'")
    print("\nè¾“å…¥ 'quit' é€€å‡ºå¯¹è¯")
    
    # å¯¹è¯å¾ªç¯
    while True:
        print("\n" + "="*50)
        user_input = input("ğŸ‘¤ ä½ ï¼š").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'å†è§']:
            print("ğŸ¤– åŠ©æ‰‹ï¼šå†è§ï¼æ„Ÿè°¢ä½¿ç”¨å›¾ç‰‡æœç´¢æœåŠ¡ï¼")
            break
        
        if not user_input:
            print("ğŸ¤– åŠ©æ‰‹ï¼šè¯·å‘Šè¯‰æˆ‘ä½ æƒ³è¦ä»€ä¹ˆæ ·çš„å›¾ç‰‡ï¼Ÿ")
            continue
        
        print("ğŸ¤– åŠ©æ‰‹ï¼šè®©æˆ‘æ¥å¸®ä½ æœç´¢...")
        
        # ä½¿ç”¨AIè§£æç”¨æˆ·æŸ¥è¯¢
        parsed_query = parse_user_query_with_ai(user_input)
        
        print(f"ğŸ” {parsed_query['response_message']}")
        
        # æ‰§è¡Œæœç´¢
        search_results = search_images_with_dedup(
            chroma_all, 
            parsed_query['search_query'],
            format_filter=parsed_query['format_filter'],
            max_results=5
        )
        
        # ä½¿ç”¨AIæ ¼å¼åŒ–ç»“æœ
        formatted_response = format_search_results_with_ai(search_results, user_input)
        print(f"\nğŸ¤– åŠ©æ‰‹ï¼š{formatted_response}")

if __name__ == "__main__":
    main()