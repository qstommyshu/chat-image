"""
å¤šHTMLæ–‡ä»¶å›¾ç‰‡æœç´¢è„šæœ¬ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
æ”¯æŒåŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰HTMLæ–‡ä»¶åˆ°Chromaå‘é‡æ•°æ®åº“
åŒ…å«å®Œæ•´çš„å»é‡é€»è¾‘
"""
import os
import glob
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# è½½å…¥ç¯å¢ƒå˜é‡
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")

def filename_to_url(filename):
    """å°†æ–‡ä»¶åè½¬æ¢å›åŸå§‹URL"""
    # ç§»é™¤.htmlåç¼€
    name_without_ext = filename.replace('.html', '')
    
    # å°†_æ›¿æ¢ä¸º/
    url_path = name_without_ext.replace('_', '/')
    
    # æ·»åŠ https://å‰ç¼€
    if url_path.startswith('www.'):
        return f"https://{url_path}"
    else:
        return f"https://{url_path}"

def get_image_format(url):
    """è·å–å›¾ç‰‡æ ¼å¼"""
    url_lower = url.lower()
    if any(ext in url_lower for ext in ['.jpg', '.jpeg']):
        return 'jpg'
    elif '.png' in url_lower:
        return 'png'
    elif '.svg' in url_lower:
        return 'svg'
    elif '.webp' in url_lower:
        return 'webp'
    elif '.gif' in url_lower:
        return 'gif'
    else:
        return 'unknown'

def extract_context_from_source(source_tag):
    """ä»sourceæ ‡ç­¾æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    context_parts = []
    
    # 1. sourceæ ‡ç­¾çš„å±æ€§
    media_attr = source_tag.get('media', '')
    if media_attr:
        context_parts.append(f"Media: {media_attr}")
    
    # 2. æŸ¥æ‰¾å…³è”çš„pictureå…ƒç´ 
    picture = source_tag.find_parent('picture')
    if picture:
        img_in_picture = picture.find('img')
        if img_in_picture:
            alt_text = img_in_picture.get('alt', '')
            title_text = img_in_picture.get('title', '')
            class_attr = ' '.join(img_in_picture.get('class', []))
            
            if alt_text:
                context_parts.append(f"Alt: {alt_text}")
            if title_text:
                context_parts.append(f"Title: {title_text}")
            if class_attr:
                context_parts.append(f"Class: {class_attr}")
    
    # 3. æŸ¥æ‰¾çˆ¶å…ƒç´ çš„æ–‡æœ¬å†…å®¹
    parent = source_tag.parent
    if parent:
        parent_text = parent.get_text(strip=True)
        if parent_text and len(parent_text) < 300:
            context_parts.append(f"Parent text: {parent_text}")
    
    return " | ".join(context_parts) if context_parts else str(source_tag)

def extract_context(img_tag):
    """æå–å›¾ç‰‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    context_parts = []
    
    # 1. imgæ ‡ç­¾æœ¬èº«çš„å±æ€§
    alt_text = img_tag.get('alt', '')
    title_text = img_tag.get('title', '')
    class_attr = ' '.join(img_tag.get('class', []))
    
    if alt_text:
        context_parts.append(f"Alt: {alt_text}")
    if title_text:
        context_parts.append(f"Title: {title_text}")
    if class_attr:
        context_parts.append(f"Class: {class_attr}")
    
    # 2. çˆ¶å…ƒç´ çš„æ–‡æœ¬å†…å®¹
    parent = img_tag.parent
    if parent:
        parent_text = parent.get_text(strip=True)
        if parent_text and len(parent_text) < 200:
            context_parts.append(f"Parent text: {parent_text}")
    
    # 3. æŸ¥æ‰¾ç›¸é‚»çš„æ–‡æœ¬å…ƒç´ 
    for sibling in img_tag.find_next_siblings(text=True)[:2]:
        sibling_text = sibling.strip()
        if sibling_text and len(sibling_text) < 100:
            context_parts.append(f"Next text: {sibling_text}")
            break
    
    return " | ".join(context_parts) if context_parts else str(img_tag)

def process_html_file(html_file_path, source_url):
    """å¤„ç†å•ä¸ªHTMLæ–‡ä»¶ï¼Œè¿”å›æ–‡æ¡£åˆ—è¡¨"""
    print(f"\nå¤„ç†æ–‡ä»¶: {os.path.basename(html_file_path)}")
    print(f"æ¥æºURL: {source_url}")
    
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html = f.read()
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        try:
            with open(html_file_path, 'r', encoding='latin-1') as f:
                html = f.read()
        except:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {html_file_path}")
            return []
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # è§£æbase_urlç”¨äºç›¸å¯¹è·¯å¾„è½¬æ¢
    parsed_url = urlparse(source_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    docs = []
    all_imgs = soup.find_all('img')
    all_sources = soup.find_all('source')
    
    print(f"  æ‰¾åˆ° {len(all_imgs)} ä¸ªimgæ ‡ç­¾")
    print(f"  æ‰¾åˆ° {len(all_sources)} ä¸ªsourceæ ‡ç­¾")
    
    # å¤„ç†imgæ ‡ç­¾
    for img in all_imgs:
        raw = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-srcset')
        if not raw:
            continue
        
        for part in raw.split(','):
            u = part.strip().split(' ')[0]
            if u.startswith('//'):
                u = 'https:' + u
            elif u.startswith('/'):
                u = urljoin(base_url, u)
            elif not u.startswith('http'):
                u = urljoin(source_url, u)
            
            img_format = get_image_format(u)
            context = extract_context(img)
            
            # æå–imgæ ‡ç­¾çš„å±æ€§
            alt_text = img.get('alt', '')
            title_text = img.get('title', '')
            class_attr = ' '.join(img.get('class', []))
            
            doc = Document(
                page_content=f"Alt: {alt_text} | Title: {title_text} | Class: {class_attr} | Context: {context}",
                metadata={
                    'img_url': u,
                    'img_format': img_format,
                    'alt_text': alt_text,
                    'title': title_text,
                    'class': class_attr,
                    'source_type': 'img',
                    'source_url': source_url,  # è®°å½•æ¥æºé¡µé¢
                    'source_file': os.path.basename(html_file_path)  # è®°å½•æ–‡ä»¶å
                }
            )
            docs.append(doc)
    
    # å¤„ç†sourceæ ‡ç­¾
    for source in all_sources:
        srcset = source.get('srcset', '')
        if not srcset:
            continue
        
        for part in srcset.split(','):
            url_part = part.strip().split(' ')[0]
            if url_part.startswith('/'):
                url_part = urljoin(base_url, url_part)
            elif not url_part.startswith('http'):
                url_part = urljoin(source_url, url_part)
            
            img_format = get_image_format(url_part)
            context = extract_context_from_source(source)
            
            # å°è¯•ä»å…³è”çš„picture/imgè·å–æ›´å¤šä¿¡æ¯
            picture = source.find_parent('picture')
            alt_text = ''
            title_text = ''
            class_attr = ''
            
            if picture:
                img_in_picture = picture.find('img')
                if img_in_picture:
                    alt_text = img_in_picture.get('alt', '')
                    title_text = img_in_picture.get('title', '')
                    class_attr = ' '.join(img_in_picture.get('class', []))
            
            doc = Document(
                page_content=f"Alt: {alt_text} | Title: {title_text} | Class: {class_attr} | Context: {context}",
                metadata={
                    'img_url': url_part,
                    'img_format': img_format,
                    'alt_text': alt_text,
                    'title': title_text,
                    'class': class_attr,
                    'source_type': 'source',
                    'media': source.get('media', ''),
                    'source_url': source_url,  # è®°å½•æ¥æºé¡µé¢
                    'source_file': os.path.basename(html_file_path)  # è®°å½•æ–‡ä»¶å
                }
            )
            docs.append(doc)
    
    print(f"  æå–äº† {len(docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    return docs

def load_html_folder(folder_path):
    """åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰HTMLæ–‡ä»¶"""
    print(f"\n=== åŠ è½½HTMLæ–‡ä»¶å¤¹: {folder_path} ===")
    
    if not os.path.exists(folder_path):
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
    html_pattern = os.path.join(folder_path, "*.html")
    html_files = glob.glob(html_pattern)
    
    if not html_files:
        raise ValueError(f"åœ¨æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶: {folder_path}")
    
    print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    all_docs = []
    source_stats = {}
    format_stats = {}
    
    for html_file in html_files:
        filename = os.path.basename(html_file)
        source_url = filename_to_url(filename)
        
        # å¤„ç†å•ä¸ªHTMLæ–‡ä»¶
        docs = process_html_file(html_file, source_url)
        all_docs.extend(docs)
        
        # ç»Ÿè®¡æ¥æº
        source_stats[source_url] = len(docs)
        
        # ç»Ÿè®¡æ ¼å¼
        for doc in docs:
            fmt = doc.metadata['img_format']
            format_stats[fmt] = format_stats.get(fmt, 0) + 1
    
    print(f"\n=== åŠ è½½å®Œæˆ ===")
    print(f"æ€»å…±å¤„ç†äº† {len(all_docs)} ä¸ªå›¾ç‰‡æ–‡æ¡£")
    print(f"æ¥è‡ª {len(html_files)} ä¸ªä¸åŒçš„é¡µé¢")
    
    print(f"\n=== æ¥æºç»Ÿè®¡ ===")
    for source_url, count in sorted(source_stats.items()):
        print(f"  {source_url}: {count} å¼ å›¾ç‰‡")
    
    print(f"\n=== æ ¼å¼ç»Ÿè®¡ ===")
    for fmt, count in sorted(format_stats.items()):
        print(f"  {fmt}: {count} å¼ ")
    
    return all_docs

# ä¸»ç¨‹åº
def main():
    # é…ç½®æ–‡ä»¶å¤¹è·¯å¾„
    crawled_folder = "crawled_pages_apple"  # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„
    
    # åŠ è½½æ‰€æœ‰HTMLæ–‡ä»¶
    all_docs = load_html_folder(crawled_folder)
    
    if not all_docs:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡æ¡£ï¼")
        return
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    print(f"\n=== åˆ›å»ºå‘é‡å­˜å‚¨ ===")
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    
    chroma_all = Chroma.from_documents(
        all_docs,
        embedding=embeddings,
        collection_name='multi_page_images'
    )
    
    print(f"å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼")
    
    # ç»Ÿè®¡å„ç§æ ¼å¼
    jpg_docs = [doc for doc in all_docs if doc.metadata['img_format'] == 'jpg']
    png_docs = [doc for doc in all_docs if doc.metadata['img_format'] == 'png']
    print(f"å…¶ä¸­JPGæ ¼å¼: {len(jpg_docs)} ä¸ª")
    print(f"å…¶ä¸­PNGæ ¼å¼: {len(png_docs)} ä¸ª")
    
    # äº¤äº’å¼æŸ¥è¯¢ç³»ç»Ÿ
    print(f"\n=== å¤šé¡µé¢å›¾ç‰‡æœç´¢ç³»ç»Ÿ ===")
    print("è¾“å…¥æŸ¥è¯¢æ¥æœç´¢ç›¸å…³å›¾ç‰‡ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("\nğŸ” æœç´¢æ ¼å¼:")
    print("  your_query          - åŸºäºAltæ–‡æœ¬æœç´¢æ‰€æœ‰æ ¼å¼ï¼ˆä¼˜å…ˆJPG/PNGï¼‰")
    print("  jpg:your_query      - åŸºäºAltæ–‡æœ¬ä»…æœç´¢JPGæ ¼å¼")
    print("  png:your_query      - åŸºäºAltæ–‡æœ¬ä»…æœç´¢PNGæ ¼å¼")
    print("  jpg+png:your_query  - åŸºäºAltæ–‡æœ¬ä»…æœç´¢JPGå’ŒPNGæ ¼å¼")
    print("\nğŸ’¡ ç¤ºä¾‹æŸ¥è¯¢:")
    print("  apple pencil        - åœ¨æ‰€æœ‰é¡µé¢ä¸­æŸ¥æ‰¾Apple Pencilç›¸å…³å›¾ç‰‡")
    print("  jpg:iPad Pro        - æŸ¥æ‰¾JPGæ ¼å¼çš„iPad Proå›¾ç‰‡")
    print("  png:iPhone camera   - æŸ¥æ‰¾PNGæ ¼å¼çš„iPhoneç›¸æœºå›¾ç‰‡")
    print("\nğŸ“ æœç´¢è¯´æ˜:")
    print("  - æœç´¢ç»“æœä¼šæ˜¾ç¤ºå›¾ç‰‡æ¥æºé¡µé¢URL")
    print("  - è‡ªåŠ¨å»é‡ç›¸åŒAltæ–‡æœ¬å’Œç›¸åŒæ–‡ä»¶çš„ä¸åŒå°ºå¯¸")
    print("  - æ”¯æŒè·¨å¤šä¸ªé¡µé¢æœç´¢")
    
    while True:
        user_input = input("\nè¯·è¾“å…¥æœç´¢æŸ¥è¯¢: ").strip()
        
        if user_input.lower() == 'quit':
            print("é€€å‡ºæœç´¢ã€‚å†è§ï¼")
            break
            
        if not user_input:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚")
            continue
        
        # è§£ææœç´¢æ ¼å¼å’ŒæŸ¥è¯¢
        format_filter = None
        query = user_input
        
        if user_input.startswith('jpg:'):
            format_filter = ['jpg']
            query = user_input[4:].strip()
        elif user_input.startswith('png:'):
            format_filter = ['png']
            query = user_input[4:].strip()
        elif user_input.startswith('jpg+png:'):
            format_filter = ['jpg', 'png']
            query = user_input[8:].strip()
        
        if not query:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢è¯ã€‚")
            continue
        
        # æ˜¾ç¤ºæœç´¢ä¿¡æ¯
        if format_filter:
            format_str = " + ".join(format_filter).upper()
            print(f"\nğŸ” æ­£åœ¨è·¨é¡µé¢æœç´¢ {format_str} æ ¼å¼çš„å›¾ç‰‡: '{query}'...")
        else:
            print(f"\nğŸ” æ­£åœ¨è·¨é¡µé¢æœç´¢æ‰€æœ‰æ ¼å¼çš„å›¾ç‰‡: '{query}' (ä¼˜å…ˆJPG/PNG)...")
        
        # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
        results = chroma_all.similarity_search_with_score(query, k=50)
        
        print(f"ğŸ” ä»å‘é‡æ•°æ®åº“æ‰¾åˆ° {len(results)} ä¸ªåˆå§‹åŒ¹é…ç»“æœ")
        
        # å¤„ç†å’Œè¿‡æ»¤ç»“æœ
        processed_results = []
        
        for doc, score in results:
            img_format = doc.metadata['img_format']
            
            # åº”ç”¨æ ¼å¼è¿‡æ»¤
            if format_filter and img_format not in format_filter:
                continue
            
            # æå–Altæ–‡æœ¬è¿›è¡Œé¢å¤–åŒ¹é…æ£€æŸ¥
            alt_text = doc.metadata.get('alt_text', '').lower()
            title_text = doc.metadata.get('title', '').lower()
            query_lower = query.lower()
            
            # è®¡ç®—Altæ–‡æœ¬åŒ¹é…åº¦
            alt_match_score = 0
            if alt_text and query_lower in alt_text:
                alt_match_score += 2.0
            if title_text and query_lower in title_text:
                alt_match_score += 1.0
            
            query_words = query_lower.split()
            for word in query_words:
                if len(word) > 2:
                    if word in alt_text:
                        alt_match_score += 0.5
                    if word in title_text:
                        alt_match_score += 0.3
            
            img_info = {
                'url': doc.metadata['img_url'],
                'format': img_format,
                'alt_text': doc.metadata.get('alt_text', ''),
                'title': doc.metadata.get('title', ''),
                'source_type': doc.metadata['source_type'],
                'media': doc.metadata.get('media', ''),
                'score': score,
                'alt_match_score': alt_match_score,
                'source_url': doc.metadata['source_url'],  # æ¥æºé¡µé¢URL
                'source_file': doc.metadata['source_file'],  # æ¥æºæ–‡ä»¶å
                'context': doc.page_content
            }
            processed_results.append(img_info)
        
        print(f"ğŸ¯ åº”ç”¨æ ¼å¼è¿‡æ»¤å: {len(processed_results)} ä¸ªç»“æœ")
        
        # å»é‡é€»è¾‘ï¼š1) åŸºäºæ–‡ä»¶å 2) åŸºäºAltæ–‡æœ¬
        def get_image_base_name(url):
            """æå–å›¾ç‰‡çš„åŸºç¡€åç§°ï¼Œç”¨äºæ£€æµ‹é‡å¤"""
            import re
            from urllib.parse import urlparse
            
            path = urlparse(url).path
            filename = path.split('/')[-1]
            name_without_ext = filename.rsplit('.', 1)[0]
            
            # ç§»é™¤å¸¸è§çš„å¤§å°/åˆ†è¾¨ç‡åç¼€
            patterns_to_remove = [
                r'_2x$',           # _2x
                r'_3x$',           # _3x
                r'_large$',        # _large
                r'_medium$',       # _medium
                r'_small$',        # _small
                r'_thumb$',        # _thumb
                r'_\d+x\d+$',      # _1920x1080
                r'_@2x$',          # _@2x
                r'_@3x$',          # _@3x
            ]
            
            base_name = name_without_ext
            for pattern in patterns_to_remove:
                base_name = re.sub(pattern, '', base_name)
            
            path_parts = path.split('/')[:-1]
            if len(path_parts) > 2:
                context_path = '/'.join(path_parts[-2:])
                return f"{context_path}/{base_name}"
            else:
                return base_name
        
        def should_prefer_image(img1, img2):
            """å†³å®šåº”è¯¥ä¿ç•™å“ªå¼ å›¾ç‰‡"""
            url1, url2 = img1['url'], img2['url']
            
            # 1. ä¼˜å…ˆä¿ç•™éretinaç‰ˆæœ¬
            has_retina_1 = any(suffix in url1.lower() for suffix in ['_2x', '_3x', '@2x', '@3x'])
            has_retina_2 = any(suffix in url2.lower() for suffix in ['_2x', '_3x', '@2x', '@3x'])
            
            if has_retina_1 != has_retina_2:
                return not has_retina_1
            
            # 2. ä¼˜å…ˆä¿ç•™æ›´å¤§å°ºå¯¸
            size_priority = {'large': 3, 'medium': 2, 'small': 1, '': 2}
            
            def get_size_priority(url):
                url_lower = url.lower()
                for size in size_priority:
                    if size and size in url_lower:
                        return size_priority[size]
                return size_priority['']
            
            priority1 = get_size_priority(url1)
            priority2 = get_size_priority(url2)
            
            if priority1 != priority2:
                return priority1 > priority2
            
            # 3. ä¿ç•™AltåŒ¹é…åº¦æ›´é«˜çš„
            if img1['alt_match_score'] != img2['alt_match_score']:
                return img1['alt_match_score'] > img2['alt_match_score']
            
            # 4. ä¿ç•™ç›¸ä¼¼åº¦åˆ†æ•°æ›´é«˜çš„
            return img1['score'] < img2['score']
        
        # ç¬¬ä¸€å±‚å»é‡ï¼šåŸºäºæ–‡ä»¶å
        unique_results = []
        seen_base_names = {}
        
        for img in processed_results:
            base_name = get_image_base_name(img['url'])
            
            if base_name in seen_base_names:
                existing_img = seen_base_names[base_name]
                if should_prefer_image(img, existing_img):
                    seen_base_names[base_name] = img
                    unique_results = [r for r in unique_results if get_image_base_name(r['url']) != base_name]
                    unique_results.append(img)
            else:
                seen_base_names[base_name] = img
                unique_results.append(img)
        
        print(f"ğŸ“‹ æ–‡ä»¶åå»é‡: {len(processed_results)} -> {len(unique_results)} ä¸ªç»“æœ")
        
        # ç¬¬äºŒå±‚å»é‡ï¼šåŸºäºAltæ–‡æœ¬
        def normalize_alt_text(alt_text):
            """æ ‡å‡†åŒ–Altæ–‡æœ¬ï¼Œç”¨äºæ¯”è¾ƒ"""
            if not alt_text:
                return ""
            import re
            normalized = alt_text.lower().strip()
            normalized = re.sub(r'[^\w\s]', ' ', normalized)
            normalized = re.sub(r'\s+', ' ', normalized).strip()
            return normalized
        
        def should_prefer_by_alt(img1, img2):
            """åŸºäºAltæ–‡æœ¬ç›¸åŒæ—¶ï¼Œå†³å®šä¿ç•™å“ªå¼ å›¾ç‰‡"""
            # 1. ä¼˜å…ˆä¿ç•™JPGæ ¼å¼
            if img1['format'] != img2['format']:
                format_priority = {'jpg': 3, 'png': 2, 'webp': 1, 'svg': 0}
                priority1 = format_priority.get(img1['format'], 0)
                priority2 = format_priority.get(img2['format'], 0)
                if priority1 != priority2:
                    return priority1 > priority2
            
            # 2. ä¼˜å…ˆä¿ç•™AltåŒ¹é…åº¦æ›´é«˜çš„
            if img1['alt_match_score'] != img2['alt_match_score']:
                return img1['alt_match_score'] > img2['alt_match_score']
            
            # 3. ä¼˜å…ˆä¿ç•™æ›´å¤§å°ºå¯¸
            def get_image_size_score(url):
                url_lower = url.lower()
                if 'large' in url_lower:
                    return 3
                elif 'medium' in url_lower:
                    return 2
                elif 'small' in url_lower:
                    return 1
                else:
                    return 2
            
            size1 = get_image_size_score(img1['url'])
            size2 = get_image_size_score(img2['url'])
            if size1 != size2:
                return size1 > size2
            
            # 4. ä¼˜å…ˆä¿ç•™ç›¸ä¼¼åº¦åˆ†æ•°æ›´é«˜çš„
            return img1['score'] < img2['score']
        
        # æ‰§è¡ŒåŸºäºAltæ–‡æœ¬çš„å»é‡
        alt_filtered_results = []
        seen_alt_texts = {}
        
        for img in unique_results:
            alt_text = normalize_alt_text(img['alt_text'])
            
            # å¦‚æœAltæ–‡æœ¬ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ 
            if not alt_text:
                alt_filtered_results.append(img)
                continue
            
            if alt_text in seen_alt_texts:
                existing_img = seen_alt_texts[alt_text]
                if should_prefer_by_alt(img, existing_img):
                    seen_alt_texts[alt_text] = img
                    alt_filtered_results = [r for r in alt_filtered_results 
                                          if normalize_alt_text(r['alt_text']) != alt_text]
                    alt_filtered_results.append(img)
            else:
                seen_alt_texts[alt_text] = img
                alt_filtered_results.append(img)
        
        # æœ€ç»ˆç»“æœ
        final_results = alt_filtered_results
        
        print(f"ğŸ”„ Altæ–‡æœ¬å»é‡: {len(unique_results)} -> {len(final_results)} ä¸ªç»“æœ")
        print(f"âœ… æ€»å»é‡æ•ˆæœ: {len(processed_results)} -> {len(final_results)} ä¸ªç»“æœ")
        
        # æ˜¾ç¤ºå»é‡ç»Ÿè®¡
        if len(seen_alt_texts) > 0:
            duplicate_alt_count = len(unique_results) - len(final_results)
            if duplicate_alt_count > 0:
                print(f"ğŸ—‘ï¸ åŸºäºAltæ–‡æœ¬å»é‡äº† {duplicate_alt_count} ä¸ªé‡å¤å›¾ç‰‡")
                
                # æ˜¾ç¤ºè¢«å»é‡çš„Altæ–‡æœ¬æ ·æœ¬
                sample_alts = [alt for alt in seen_alt_texts.keys() if alt][:3]
                if sample_alts:
                    print("å»é‡çš„Altæ–‡æœ¬æ ·æœ¬:")
                    for alt in sample_alts:
                        print(f"  ğŸ“ '{alt}'")
        
        # æ’åºç»“æœ
        if not format_filter:
            final_results.sort(key=lambda x: (
                -x['alt_match_score'],
                x['format'] not in ['jpg', 'png'],
                x['format'] != 'jpg',
                x['score']
            ))
        else:
            final_results.sort(key=lambda x: (
                -x['alt_match_score'],
                x['score']
            ))
        
        # è¿”å›å‰5ä¸ªç»“æœ
        top_5 = final_results[:5]
        
        if not top_5:
            if format_filter:
                format_str = " + ".join(format_filter).upper()
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ° {format_str} æ ¼å¼çš„ç›¸å…³å›¾ç‰‡")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å›¾ç‰‡")
            continue
        
        print(f"\næ‰¾åˆ° {len(final_results)} ä¸ªç›¸å…³ç»“æœï¼Œæ˜¾ç¤ºå‰5ä¸ª:")
        print("=" * 80)
        
        for i, img in enumerate(top_5, 1):
            print(f"\nã€å›¾ç‰‡ {i}ã€‘")
            print(f"å›¾ç‰‡URL: {img['url']}")
            print(f"ğŸ“„ æ¥æºé¡µé¢: {img['source_url']}")
            print(f"ğŸ“ æ¥æºæ–‡ä»¶: {img['source_file']}")
            
            # æ ¹æ®æ ¼å¼æ·»åŠ æ ‡è¯†
            format_display = img['format'].upper()
            if img['format'] == 'jpg':
                format_display += " âœ…"
            elif img['format'] == 'png':
                format_display += " ğŸŸ¢"
            
            print(f"æ ¼å¼: {format_display}")
            print(f"å‘é‡ç›¸ä¼¼åº¦: {img['score']:.4f}")
            print(f"AltåŒ¹é…åº¦: {img['alt_match_score']:.1f}")
            
            if img['alt_text']:
                # é«˜äº®æ˜¾ç¤ºåŒ¹é…çš„æŸ¥è¯¢è¯
                alt_display = img['alt_text']
                query_words = query.lower().split()
                for word in query_words:
                    if len(word) > 2 and word in alt_display.lower():
                        alt_display = alt_display.replace(word, f"**{word}**")
                        alt_display = alt_display.replace(word.capitalize(), f"**{word.capitalize()}**")
                print(f"Altæ–‡æœ¬: {alt_display}")
            
            if img['title']:
                print(f"æ ‡é¢˜: {img['title']}")
            
            if img['media']:
                print(f"åª’ä½“æŸ¥è¯¢: {img['media']}")
            
            print(f"æ¥æºæ ‡ç­¾: {img['source_type']}")
            
            print("-" * 60)
        
        # ç»Ÿè®¡ä¿¡æ¯
        format_count = {}
        source_count = {}
        for img in top_5:
            fmt = img['format']
            format_count[fmt] = format_count.get(fmt, 0) + 1
            
            source = img['source_url']
            source_count[source] = source_count.get(source, 0) + 1
        
        print(f"\nğŸ“Š ç»“æœæ ¼å¼åˆ†å¸ƒ: {dict(format_count)}")
        print(f"ğŸ“„ ç»“æœæ¥æºåˆ†å¸ƒ: {dict(source_count)}")

if __name__ == "__main__":
    main()